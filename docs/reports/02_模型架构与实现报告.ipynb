{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4b24b70b",
      "metadata": {},
      "source": [
        "# 02_模型架构与实现报告\n",
        "\n",
        "**项目名称**: SASRec.pytorch - 基于Transformer的序列推荐系统  \n",
        "**版本**: v1.0  \n",
        "**创建日期**: 2024-01-10  \n",
        "\n",
        "---\n",
        "\n",
        "## 目录\n",
        "\n",
        "1. [SASRec模型架构](#1-SASRec模型架构)  \n",
        "2. [TiSASRec时序感知机制](#2-TiSASRec时序感知机制)  \n",
        "3. [mHC流形约束超连接](#3-mHC流形约束超连接)  \n",
        "4. [核心代码实现](#4-核心代码实现)  \n",
        "5. [模型对比](#5-模型对比)  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. SASRec模型架构\n",
        "\n",
        "### 1.1 自注意力机制理论基础\n",
        "\n",
        "SASRec的核心是**自注意力机制（Self-Attention）**，它通过计算序列中每个位置与其他位置之间的关联强度，捕捉序列内的长期依赖关系。其数学表达式如下：\n",
        "\n",
        "**缩放点积注意力（Scaled Dot-Product Attention）**：\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "其中：\n",
        "- $Q \\in \\mathbb{R}^{n \\times d_k}$：查询矩阵（Query）\n",
        "- $K \\in \\mathbb{R}^{n \\times d_k}$：键矩阵（Key）\n",
        "- $V \\in \\mathbb{R}^{n \\times d_v}$：值矩阵（Value）\n",
        "- $d_k$：注意力头的维度\n",
        "- $\\sqrt{d_k}$：缩放因子，用于稳定梯度\n",
        "\n",
        "**多头注意力（Multi-Head Attention）**：\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}\\left(\\text{head}_1, \\ldots, \\text{head}_h\\right)W^O$$\n",
        "\n",
        "其中每个注意力头计算为：\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}\\left(QW_i^Q, KW_i^K, VW_i^V\\right)$$\n",
        "\n",
        "其中 $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{model} \\times d_k}$ 和 $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$ 为可学习的投影矩阵。\n",
        "\n",
        "**位置编码（Positional Encoding）**：\n",
        "\n",
        "由于Transformer没有循环结构，需要显式注入位置信息：\n",
        "\n",
        "$$PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "$$PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "其中 $pos$ 为位置索引，$i$ 为维度索引。\n",
        "\n",
        "### 1.2 模型概述\n",
        "\n",
        "SASRec (Self-Attentive Sequential Recommendation) 是基于Transformer的自注意力序列推荐模型。它使用多头注意力机制来捕捉用户行为序列中的长期依赖关系。\n",
        "\n",
        "**核心特点**：\n",
        "- 使用位置编码捕捉序列顺序\n",
        "- 多头注意力机制学习物品间的依赖\n",
        "- 基于物品嵌入的预测"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a6103d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, num_heads, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_units // num_heads\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to('cuda')\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        \n",
        "        # Linear projections\n",
        "        Q = self.W_Q(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.W_K(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.W_V(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, -1e9)\n",
        "        attention = self.dropout(F.softmax(attention, dim=-1))\n",
        "        \n",
        "        # Output\n",
        "        output = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\n",
        "        output = output.view(batch_size, -1, self.num_heads * self.head_dim)\n",
        "        output = self.W_O(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5c20f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointWiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PointWiseFeedForward, self).__init__()\n",
        "        self.W_1 = nn.Linear(hidden_units, hidden_units, bias=True)\n",
        "        self.W_2 = nn.Linear(hidden_units, hidden_units, bias=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.W_2(F.relu(self.W_1(inputs)))\n",
        "        output = self.dropout(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986838e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SASRecBlock(nn.Module):\n",
        "    def __init__(self, hidden_units, num_heads, dropout_rate):\n",
        "        super(SASRecBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(hidden_units, num_heads, dropout_rate)\n",
        "        self.ffn = PointWiseFeedForward(hidden_units, dropout_rate)\n",
        "        self.layernorm1 = nn.LayerNorm(hidden_units, eps=1e-8)\n",
        "        self.layernorm2 = nn.LayerNorm(hidden_units, eps=1e-8)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input_emb, mask):\n",
        "        attn_output = self.mha(input_emb, input_emb, input_emb, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(input_emb + attn_output)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a236dc2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SASRec(nn.Module):\n",
        "    def __init__(self, item_num, hidden_units=50, num_blocks=2, num_heads=1, dropout_rate=0.2, maxlen=100):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.item_num = item_num\n",
        "        self.hidden_units = hidden_units\n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "        self.item_embeddings = nn.Embedding(item_num + 1, hidden_units, padding_idx=0)\n",
        "        self.pos_embeddings = nn.Embedding(maxlen + 1, hidden_units)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([\n",
        "            SASRecBlock(hidden_units, num_heads, dropout_rate) \n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "        \n",
        "        self.LayerNorm = nn.LayerNorm(hidden_units, eps=1e-8)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0, std=0.01)\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, input_seq, mask):\n",
        "        seq_emb = self.item_embeddings(input_seq)\n",
        "        positions = torch.arange(input_seq.size(1), dtype=torch.long, device=input_seq.device)\n",
        "        pos_emb = self.pos_embeddings(positions)\n",
        "        \n",
        "        emb = seq_emb + pos_emb\n",
        "        emb = self.dropout(self.LayerNorm(emb))\n",
        "        \n",
        "        for block in self.blocks:\n",
        "            emb = block(emb, mask)\n",
        "        \n",
        "        return emb\n",
        "    \n",
        "    def predict(self, input_seq, target_items, mask):\n",
        "        seq_emb = self.forward(input_seq, mask)\n",
        "        last_emb = seq_emb[:, -1, :]\n",
        "        target_emb = self.item_embeddings(target_items)\n",
        "        scores = torch.matmul(last_emb, target_emb.transpose(0, 1))\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d7b744c",
      "metadata": {},
      "source": [
        "## 2. TiSASRec时序感知机制\n",
        "\n",
        "### 2.1 时间间隔建模理论基础\n",
        "\n",
        "TiSASRec在SASRec基础上引入**时间间隔信息**，相邻交互的时间间隔被编码并融入注意力计算。时间间隔的编码方式如下：\n",
        "\n",
        "**时间间隔编码公式**：\n",
        "\n",
        "$$t_{ij} = \\log\\left(1 + \\Delta_{ij}\\right) / \\log\\left(1 + T_{max}\\right)$$\n",
        "\n",
        "其中：\n",
        "- $\\Delta_{ij} = |t_i - t_j|$：相邻交互之间的时间差\n",
        "- $t_i, t_j$ 分别为第 $i$ 和 $j$ 个交互的时间戳\n",
        "- $T_{max}$：预设的最大时间间隔阈值（默认30天 = 86400×30秒）\n",
        "- $\\log(1 + \\cdot)$：对数变换，用于压缩时间跨度范围\n",
        "\n",
        "**带时间间隔的注意力计算**：\n",
        "\n",
        "$$\\text{Attention}_{time}(Q, K, V, T) = \\text{softmax}\\left(\\frac{QK^T + \\alpha \\cdot T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "其中 $T$ 为时间间隔矩阵，$\\alpha$ 为时间间隔的权重系数。\n",
        "\n",
        "### 2.2 时间间隔建模\n",
        "\n",
        "TiSASRec在SASRec基础上引入时间间隔信息，相邻交互的时间间隔被编码并融入注意力计算。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7482b0ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_time_matrix(timestamps, max_time_gap=86400 * 30):\n",
        "    \"\"\"计算相邻交互之间的时间间隔矩阵\"\"\"\n",
        "    batch_size, seq_len = timestamps.shape\n",
        "    time_diffs = timestamps.unsqueeze(2) - timestamps.unsqueeze(1)\n",
        "    time_diffs = torch.log1p(torch.clamp(time_diffs, min=0)) / (torch.log1p(torch.tensor(max_time_gap)) + 1e-8)\n",
        "    return time_diffs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad7212a",
      "metadata": {},
      "source": [
        "## 3. mHC流形约束超连接\n",
        "\n",
        "### 3.1 Sinkhorn-Knopp算法理论基础\n",
        "\n",
        "mHC（Manifold Hyperconnection）创新性地引入**Sinkhorn-Knopp算法**，将权重矩阵投影到**双随机矩阵流形**上，实现流形约束的正则化。\n",
        "\n",
        "**双随机矩阵定义**：\n",
        "\n",
        "矩阵 $P \\in \\mathbb{R}^{n \\times n}$ 被称为双随机矩阵，当且仅当：\n",
        "\n",
        "$$P \\mathbf{1} = \\mathbf{1} \\quad \\text{且} \\quad P^T \\mathbf{1} = \\mathbf{1}$$\n",
        "\n",
        "即矩阵的每一行和每一列的和均为1，且所有元素非负。\n",
        "\n",
        "**Sinkhorn-Knopp迭代算法**：\n",
        "\n",
        "通过交替缩放行和列，将任意非负矩阵 $M$ 收敛到双随机矩阵 $P$：\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\text{初始化: } P^{(0)} = M + \\epsilon \\\\\n",
        "&\\text{迭代 } k = 1, 2, \\ldots, K: \\\\\n",
        "&\\quad P^{(k)}_{\\text{row}} = \\frac{P^{(k-1)}}{P^{(k-1)} \\mathbf{1} \\mathbf{1}^T} \\quad \\text{（行归一化）} \\\\\n",
        "&\\quad P^{(k)} = \\frac{P^{(k)}_{\\text{row}}}{\\mathbf{1} \\mathbf{1}^T P^{(k)}_{\\text{row}}} \\quad \\text{（列归一化）}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "其中分数形式表示逐元素除法，$\\mathbf{1}$ 为全1向量。\n",
        "\n",
        "**mHC层的前向传播**：\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&H^{(h)} = \\text{Sinkhorn}\\left(W^{(h)}\\right) \\quad \\forall h \\in \\{1, \\ldots, h_{num}\\\\\n",
        "&\\text{Head}^{(h)} = X \\cdot H^{(h)} \\\\\n",
        "&Y = \\text{Projection}\\left(\\frac{1}{h_{num}} \\sum_{h=1}^{h_{num}} \\text{Head}^{(h)}\\right) \\\\\n",
        "&\\text{Output} = X + Y\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "### 3.2 Sinkhorn-Knopp算法\n",
        "\n",
        "Sinkhorn-Knopp算法通过交替缩放行和列，将任意非负矩阵转换为双随机矩阵。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd1a684",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sinkhorn_knopp(M, num_iterations=100, eps=1e-6):\n",
        "    \"\"\"Sinkhorn-Knopp算法：将矩阵投影到双随机矩阵流形\"\"\"\n",
        "    n = M.shape[0]\n",
        "    P = M + eps\n",
        "    \n",
        "    for _ in range(num_iterations):\n",
        "        row_sums = P.sum(dim=1, keepdim=True)\n",
        "        P = P / (row_sums + 1e-10)\n",
        "        col_sums = P.sum(dim=0, keepdim=True)\n",
        "        P = P / (col_sums + 1e-10)\n",
        "    \n",
        "    return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea026ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MHCLayer(nn.Module):\n",
        "    def __init__(self, hidden_units, num_heads=4, dropout_rate=0.1):\n",
        "        super(MHCLayer, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.hyper_weights = nn.Parameter(torch.randn(num_heads, hidden_units, hidden_units))\n",
        "        self.out_proj = nn.Linear(hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, inputs, mask=None):\n",
        "        batch_size, seq_len, hidden_units = inputs.shape\n",
        "        \n",
        "        hyper_proj = []\n",
        "        for h in range(self.num_heads):\n",
        "            W = self.hyper_weights[h]\n",
        "            W_proj = sinkhorn_knopp(W, num_iterations=20)\n",
        "            hyper_proj.append(W_proj)\n",
        "        \n",
        "        head_outputs = []\n",
        "        for h in range(self.num_heads):\n",
        "            output = torch.matmul(inputs, hyper_proj[h])\n",
        "            if mask is not None:\n",
        "                output = output.masked_fill(mask.unsqueeze(-1) == 0, 0)\n",
        "            head_outputs.append(output)\n",
        "        \n",
        "        combined = torch.stack(head_outputs, dim=-1)\n",
        "        combined = combined.mean(dim=-1)\n",
        "        output = self.out_proj(combined)\n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        return inputs + output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e9af0bc",
      "metadata": {},
      "source": [
        "## 4. 核心代码实现\n",
        "\n",
        "### 4.1 模型组件对比\n",
        "\n",
        "| 组件 | SASRec | TiSASRec | mHC |\n",
        "|------|--------|----------|-----|\n",
        "| 位置编码 | Learnable | Learnable | Learnable |\n",
        "| 时间编码 | None | TimeInterval | None |\n",
        "| 注意力 | MultiHead | MultiHead+Time | MultiHead+mHC |\n",
        "\n",
        "### 4.2 训练流程"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdbd193",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        user_ids, seq, pos_items, neg_items, mask = batch\n",
        "        seq = seq.to(device)\n",
        "        pos_items = pos_items.to(device)\n",
        "        neg_items = neg_items.to(device)\n",
        "        mask = mask.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        seq_emb = model(seq, mask)\n",
        "        \n",
        "        pos_emb = model.item_embeddings(pos_items)\n",
        "        pos_scores = (seq_emb[:, -1, :] * pos_emb).sum(dim=-1)\n",
        "        \n",
        "        neg_emb = model.item_embeddings(neg_items)\n",
        "        neg_scores = (seq_emb[:, -1, :] * neg_emb).sum(dim=-1)\n",
        "        \n",
        "        loss = criterion(pos_scores, neg_scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45487c38",
      "metadata": {},
      "source": [
        "## 5. 模型对比\n",
        "\n",
        "### 5.1 模型结构对比\n",
        "\n",
        "| 维度 | SASRec | TiSASRec | SASRec+mHC | TiSASRec+mHC |\n",
        "|------|--------|----------|------------|--------------|\n",
        "| 嵌入维度 | 50 | 50 | 50 | 50 |\n",
        "| Transformer层数 | 2 | 2 | 2 | 2 |\n",
        "| 注意力头数 | 2 | 2 | 2 | 2 |\n",
        "\n",
        "---\n",
        "\n",
        "**上一章**: [01_数据与实验设计报告.ipynb](./01_数据与实验设计报告.ipynb)  \n",
        "**下一章**: [03_训练与评估报告.ipynb](./03_训练与评估报告.ipynb)"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}