{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02_模型架构与实现报告\n",
    "\n",
    "**项目名称**: SASRec.pytorch - 基于Transformer的序列推荐系统  \n",
    "**版本**: v1.0  \n",
    "**创建日期**: 2024-01-10  \n",
    "\n",
    "---\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [模型概览](#1-模型概览)  \n",
    "2. [Transformer原理](#2-transformer原理)  \n",
    "3. [SASRec模型实现](#3-sasrec模型实现)  \n",
    "4. [TiSASRec时序感知机制](#4-tisasrec时序感知机制)  \n",
    "5. [mHC流形约束超连接](#5-mhc流形约束超连接)  \n",
    "6. [代码结构](#6-代码结构)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. 模型概览\n",
    "\n",
    "### 1.1 整体架构\n",
    "\n",
    "```\n",
    "输入序列: [物品A, 物品B, 物品C, 物品D]\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────┐\n",
    "│   物品嵌入层     │ 嵌入维度: (batch, seq_len, hidden_units)\n",
    "└─────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────┐\n",
    "│   位置嵌入层     │ 加入序列位置信息\n",
    "└─────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────┐\n",
    "│ Transformer块×2 │ 自注意力学习序列依赖\n",
    "│ (num_blocks)    │\n",
    "└─────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────┐\n",
    "│   输出层        │ 预测下一个物品\n",
    "└─────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "输出: 每个物品的预测得分\n",
    "```\n",
    "\n",
    "### 1.2 模型变体\n",
    "\n",
    "| 模型 | 特点 | 应用场景 |\n",
    "|------|------|----------|\n",
    "| SASRec | 标准自注意力 | 通用序列推荐 |\n",
    "| TiSASRec | 加入时间间隔 | 用户行为有时间规律 |\n",
    "| SASRec/TiSASRec + mHC | 流形约束超连接 | 深层网络训练 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Transformer原理\n",
    "\n",
    "### 2.1 自注意力机制\n",
    "\n",
    "自注意力（Self-Attention）让序列中每个位置都能直接与所有其他位置交互。\n",
    "\n",
    "**核心公式**：\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "其中：\n",
    "- $Q$ (Query): 查询向量\n",
    "- $K$ (Key): 键向量\n",
    "- $V$ (Value): 值向量\n",
    "- $d_k$: 缩放因子，防止点积过大\n",
    "\n",
    "### 2.2 多头注意力\n",
    "\n",
    "多头注意力将Q、K、V分割成多个头，分别计算注意力，然后拼接：\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(head_1, ..., head_h)W^O$$\n",
    "\n",
    "### 2.3 位置编码\n",
    "\n",
    "由于Transformer没有循环结构，需要显式加入位置信息：\n",
    "\n",
    "$$\\begin{aligned}\n",
    "PE_{(pos, 2i)} &= \\sin(pos / 10000^{2i/d_{model}}) \\\\\n",
    "PE_{(pos, 2i+1)} &= \\cos(pos / 10000^{2i/d_{model}})\n",
    "\\end{aligned}$$\n",
    "\n",
    "本项目使用可学习的绝对位置嵌入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 多头注意力实现\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_units // num_heads\n",
    "        \n",
    "        # Q, K, V 的线性变换\n",
    "        self.q_w = nn.Linear(hidden_units, hidden_units)\n",
    "        self.k_w = nn.Linear(hidden_units, hidden_units)\n",
    "        self.v_w = nn.Linear(hidden_units, hidden_units)\n",
    "        self.out_w = nn.Linear(hidden_units, hidden_units)\n",
    "    \n",
    "    def forward(self, queries, keys, values, attn_mask=None):\n",
    "        batch_size, seq_len, hidden_units = queries.shape\n",
    "        \n",
    "        # 线性变换\n",
    "        Q = self.q_w(queries)\n",
    "        K = self.k_w(keys)\n",
    "        V = self.v_w(values)\n",
    "        \n",
    "        # 分割成多个头\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_size).transpose(1, 2)\n",
    "        \n",
    "        # 计算注意力分数\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.head_size ** 0.5)\n",
    "        \n",
    "        # 应用掩码\n",
    "        if attn_mask is not None:\n",
    "            scores = scores.masked_fill(attn_mask, -1e9)\n",
    "        \n",
    "        # Softmax归一化\n",
    "        weights = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # 加权求和\n",
    "        output = torch.matmul(weights, V)\n",
    "        \n",
    "        # 合并多个头\n",
    "        output = output.transpose(1, 2).contiguous()\n",
    "        output = output.view(batch_size, seq_len, hidden_units)\n",
    "        \n",
    "        return self.out_w(output)\n",
    "\n",
    "# 测试多头注意力\n",
    "batch_size, seq_len, hidden_units, num_heads = 2, 10, 50, 2\n",
    "attn = MultiHeadAttention(hidden_units, num_heads)\n",
    "\n",
    "x = torch.randn(batch_size, seq_len, hidden_units)\n",
    "output = attn(x, x, x)\n",
    "\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SASRec模型实现\n",
    "\n",
    "### 3.1 模型定义\n",
    "\n",
    "SASRec使用Transformer编码器架构处理用户行为序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前馈网络\n",
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_units, hidden_units, kernel_size=1)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch, seq_len, hidden_units)\n",
    "        x = inputs.transpose(-1, -2)  # (batch, hidden_units, seq_len)\n",
    "        x = self.dropout(torch.relu(self.conv1(x)))\n",
    "        x = self.dropout(self.conv2(x))\n",
    "        return x.transpose(-1, -2)\n",
    "\n",
    "# SASRec模型\n",
    "class SASRec(nn.Module):\n",
    "    def __init__(self, user_num, item_num, args):\n",
    "        super().__init__()\n",
    "        self.hidden_units = args.hidden_units\n",
    "        \n",
    "        # 物品嵌入层\n",
    "        self.item_emb = nn.Embedding(item_num + 1, args.hidden_units, padding_idx=0)\n",
    "        \n",
    "        # 位置嵌入\n",
    "        self.pos_emb = nn.Embedding(args.maxlen, args.hidden_units)\n",
    "        \n",
    "        # Transformer块\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        self.forward_layers = nn.ModuleList()\n",
    "        self.layernorms = nn.ModuleList()\n",
    "        \n",
    "        for _ in range(args.num_blocks):\n",
    "            self.attention_layers.append(\n",
    "                MultiHeadAttention(args.hidden_units, args.num_heads)\n",
    "            )\n",
    "            self.forward_layers.append(\n",
    "                PointWiseFeedForward(args.hidden_units, args.dropout_rate)\n",
    "            )\n",
    "            self.layernorms.append(nn.LayerNorm(args.hidden_units))\n",
    "        \n",
    "        self.dropout = nn.Dropout(args.dropout_rate)\n",
    "    \n",
    "    def forward(self, log_seqs):\n",
    "        # 物品嵌入\n",
    "        seq_emb = self.item_emb(log_seqs)\n",
    "        seq_emb = seq_emb * (self.hidden_units ** 0.5)\n",
    "        \n",
    "        # 位置嵌入\n",
    "        positions = torch.arange(log_seqs.size(1), device=log_seqs.device)\n",
    "        pos_emb = self.pos_emb(positions)\n",
    "        \n",
    "        # 因果掩码（防止看到未来信息）\n",
    "        seq_len = log_seqs.size(1)\n",
    "        attn_mask = ~torch.tril(torch.ones(seq_len, seq_len, dtype=torch.bool))\n",
    "        \n",
    "        # Transformer块\n",
    "        for i in range(len(self.attention_layers)):\n",
    "            # 自注意力\n",
    "            Q = self.layernorms[i](seq_emb)\n",
    "            attn_out = self.attention_layers[i](Q, seq_emb, seq_emb, attn_mask)\n",
    "            seq_emb = seq_emb + attn_out\n",
    "            \n",
    "            # 前馈网络\n",
    "            ffn_in = self.layernorms[i](seq_emb)\n",
    "            ffn_out = self.forward_layers[i](ffn_in)\n",
    "            seq_emb = seq_emb + ffn_out\n",
    "        \n",
    "        return seq_emb\n",
    "\n",
    "# 创建模型参数\n",
    "class Args:\n",
    "    hidden_units = 50\n",
    "    num_blocks = 2\n",
    "    num_heads = 2\n",
    "    dropout_rate = 0.2\n",
    "    maxlen = 200\n",
    "\n",
    "# 测试模型\n",
    "model = SASRec(user_num=1000, item_num=3700, args=Args())\n",
    "x = torch.randint(0, 3700, (2, 50))  # batch_size=2, seq_len=50\n",
    "output = model(x)\n",
    "\n",
    "print(f\"SASRec模型参数量: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"输入形状: {x.shape}\")\n",
    "print(f\"输出形状: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. TiSASRec时序感知机制\n",
    "\n",
    "### 4.1 时间间隔编码\n",
    "\n",
    "TiSASRec在标准自注意力的基础上融入时间间隔信息。\n",
    "\n",
    "**注意力计算**：\n",
    "\n",
    "$$\n",
    "A_{ij} = \\text{softmax}\\left(\\frac{Q_i K_j^T + Q_i \\cdot PE_{abs\_pos\_i}^T + T_{ij} \\cdot Q_i}{\\sqrt{d}}\\right)\n",
    "$$\n",
    "\n",
    "### 4.2 时间矩阵\n",
    "\n",
    "对于序列中每对位置(i, j)，计算时间间隔：\n",
    "\n",
    "$$\n",
    "T_{ij} = \\min(|t_i - t_j|, \\text{time_span})\n",
    "$$\n",
    "\n",
    "时间间隔被离散化并嵌入到高维向量空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 时间感知注意力\n",
    "class TimeAwareMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, num_heads, dropout_rate, time_span):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_size = hidden_units // num_heads\n",
    "        self.time_span = time_span\n",
    "        \n",
    "        self.q_w = nn.Linear(hidden_units, hidden_units)\n",
    "        self.k_w = nn.Linear(hidden_units, hidden_units)\n",
    "        self.v_w = nn.Linear(hidden_units, hidden_units)\n",
    "        \n",
    "        # 时间矩阵嵌入\n",
    "        self.time_k_emb = nn.Embedding(time_span + 1, hidden_units)\n",
    "        self.time_v_emb = nn.Embedding(time_span + 1, hidden_units)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "    \n",
    "    forward = None  # 省略完整实现\n",
    "\n",
    "# 时间矩阵计算示例\n",
    "def compute_time_matrix(timestamps, time_span=100):\n",
    "    \"\"\"计算时间间隔矩阵\"\"\"\n",
    "    n = len(timestamps)\n",
    "    time_matrix = torch.zeros(n, n, dtype=torch.long)\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            diff = abs(timestamps[i] - timestamps[j])\n",
    "            time_matrix[i, j] = min(diff, time_span)\n",
    "    return time_matrix\n",
    "\n",
    "# 示例：3个时间戳\n",
    "timestamps = [978300760, 978300109, 978301570]\n",
    "time_matrix = compute_time_matrix(timestamps, time_span=100)\n",
    "\n",
    "print(\"时间戳（Unix）:\", timestamps)\n",
    "print(\"时间间隔矩阵（天数）:\")\n",
    "print((time_matrix / (24*3600)).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. mHC流形约束超连接\n",
    "\n",
    "### 5.1 核心思想\n",
    "\n",
    "mHC（Manifold-Constrained Hyper-Connections）扩展了传统残差连接：\n",
    "\n",
    "**传统残差**：\n",
    "$$\nx_{l+1} = x_l + F(x_l)\n",
    "$$\n",
    "\n",
    "**mHC残差**：\n",
    "$$\nx_{l+1} = H_{res} \\times x_l + H_{post}^T \\times F(H_{pre} \\times x_l)\n",
    "$$\n",
    "\n",
    "### 5.2 Sinkhorn-Knopp算法\n",
    "\n",
    "通过Sinkhorn-Knopp算法将$H_{res}$投影到双随机矩阵流形，确保信号稳定传播。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinkhorn-Knopp算法\n",
    "def sinkhorn_knopp(M, max_iter=20):\n",
    "    \"\"\"将矩阵投影到双随机矩阵流形\"\"\"\n",
    "    n = M.shape[-1]\n",
    "    M = torch.exp(M)  # 指数变换\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        # 行归一化\n",
    "        row_sum = M.sum(dim=-1, keepdim=True)\n",
    "        M = M / (row_sum + 1e-12)\n",
    "        \n",
    "        # 列归一化\n",
    "        col_sum = M.sum(dim=-2, keepdim=True)\n",
    "        M = M / (col_sum + 1e-12)\n",
    "    \n",
    "    return M\n",
    "\n",
    "# 测试双随机矩阵性质\n",
    "import torch\n",
    "\n",
    "M = torch.randn(4, 4)  # 随机初始化\n",
    "M_doubly = sinkhorn_knopp(M, max_iter=20)\n",
    "\n",
    "print(\"双随机矩阵验证：\")\n",
    "print(f\"行和: {M_doubly.sum(dim=-1).tolist()} (应接近1)\")\n",
    "print(f\"列和: {M_doubly.sum(dim=-2).tolist()} (应接近1)\")\n",
    "print(f\"最小值: {M_doubly.min():.4f} (应 >= 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 代码结构\n",
    "\n",
    "### 6.1 文件组织\n",
    "\n",
    "```\n",
    "python/\n",
    "├── main.py              # 训练入口\n",
    "├── model.py             # SASRec/TiSASRec实现\n",
    "├── model_mhc.py         # mHC变体实现\n",
    "└── utils.py             # 工具函数\n",
    "```\n",
    "\n",
    "### 6.2 核心类\n",
    "\n",
    "| 文件 | 核心类 | 说明 |\n",
    "|------|--------|------|\n",
    "| model.py | SASRec, TiSASRec | 基础模型 |\n",
    "| model_mhc.py | SASRec_mHC, TiSASRec_mHC | mHC变体 |\n",
    "| model_mhc.py | mHCResidual | mHC残差模块 |\n",
    "| utils.py | WarpSampler | 数据采样 |\n",
    "| utils.py | evaluate | 评估函数 |\n",
    "\n",
    "---\n",
    "\n",
    "**上一章**: [01_数据与实验设计报告.ipynb](./01_数据与实验设计报告.ipynb)  \n",
    "**下一章**: [03_训练与评估报告.ipynb](./03_训练与评估报告.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
