{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4b24b70b",
      "metadata": {},
      "source": [
        "# 02_\u6a21\u578b\u67b6\u6784\u4e0e\u5b9e\u73b0\u62a5\u544a\n",
        "\n",
        "**\u9879\u76ee\u540d\u79f0**: SASRec.pytorch - \u57fa\u4e8eTransformer\u7684\u5e8f\u5217\u63a8\u8350\u7cfb\u7edf  \n",
        "**\u7248\u672c**: v1.0  \n",
        "**\u521b\u5efa\u65e5\u671f**: 2024-01-10  \n",
        "\n",
        "---\n",
        "\n",
        "## \u76ee\u5f55\n",
        "\n",
        "1. [SASRec\u6a21\u578b\u67b6\u6784](#1-SASRec\u6a21\u578b\u67b6\u6784)  \n",
        "2. [TiSASRec\u65f6\u5e8f\u611f\u77e5\u673a\u5236](#2-TiSASRec\u65f6\u5e8f\u611f\u77e5\u673a\u5236)  \n",
        "3. [mHC\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5](#3-mHC\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5)  \n",
        "4. [\u6838\u5fc3\u4ee3\u7801\u5b9e\u73b0](#4-\u6838\u5fc3\u4ee3\u7801\u5b9e\u73b0)  \n",
        "5. [\u6a21\u578b\u5bf9\u6bd4](#5-\u6a21\u578b\u5bf9\u6bd4)  \n",
        "\n",
        "---\n",
        "\n",
        "## 1. SASRec\u6a21\u578b\u67b6\u6784\n",
        "\n",
        "### 1.1 \u81ea\u6ce8\u610f\u529b\u673a\u5236\u7406\u8bba\u57fa\u7840\n",
        "\n",
        "SASRec\u7684\u6838\u5fc3\u662f**\u81ea\u6ce8\u610f\u529b\u673a\u5236\uff08Self-Attention\uff09**\uff0c\u5b83\u901a\u8fc7\u8ba1\u7b97\u5e8f\u5217\u4e2d\u6bcf\u4e2a\u4f4d\u7f6e\u4e0e\u5176\u4ed6\u4f4d\u7f6e\u4e4b\u95f4\u7684\u5173\u8054\u5f3a\u5ea6\uff0c\u6355\u6349\u5e8f\u5217\u5185\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002\u5176\u6570\u5b66\u8868\u8fbe\u5f0f\u5982\u4e0b\uff1a\n",
        "\n",
        "**\u7f29\u653e\u70b9\u79ef\u6ce8\u610f\u529b\uff08Scaled Dot-Product Attention\uff09**\uff1a\n",
        "\n",
        "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "\u5176\u4e2d\uff1a\n",
        "- $Q \\in \\mathbb{R}^{n \\times d_k}$\uff1a\u67e5\u8be2\u77e9\u9635\uff08Query\uff09\n",
        "- $K \\in \\mathbb{R}^{n \\times d_k}$\uff1a\u952e\u77e9\u9635\uff08Key\uff09\n",
        "- $V \\in \\mathbb{R}^{n \\times d_v}$\uff1a\u503c\u77e9\u9635\uff08Value\uff09\n",
        "- $d_k$\uff1a\u6ce8\u610f\u529b\u5934\u7684\u7ef4\u5ea6\n",
        "- $\\sqrt{d_k}$\uff1a\u7f29\u653e\u56e0\u5b50\uff0c\u7528\u4e8e\u7a33\u5b9a\u68af\u5ea6\n",
        "\n",
        "**\u591a\u5934\u6ce8\u610f\u529b\uff08Multi-Head Attention\uff09**\uff1a\n",
        "\n",
        "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}\\left(\\text{head}_1, \\ldots, \\text{head}_h\\right)W^O$$\n",
        "\n",
        "\u5176\u4e2d\u6bcf\u4e2a\u6ce8\u610f\u529b\u5934\u8ba1\u7b97\u4e3a\uff1a\n",
        "\n",
        "$$\\text{head}_i = \\text{Attention}\\left(QW_i^Q, KW_i^K, VW_i^V\\right)$$\n",
        "\n",
        "\u5176\u4e2d $W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{model} \\times d_k}$ \u548c $W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$ \u4e3a\u53ef\u5b66\u4e60\u7684\u6295\u5f71\u77e9\u9635\u3002\n",
        "\n",
        "**\u4f4d\u7f6e\u7f16\u7801\uff08Positional Encoding\uff09**\uff1a\n",
        "\n",
        "\u7531\u4e8eTransformer\u6ca1\u6709\u5faa\u73af\u7ed3\u6784\uff0c\u9700\u8981\u663e\u5f0f\u6ce8\u5165\u4f4d\u7f6e\u4fe1\u606f\uff1a\n",
        "\n",
        "$$PE_{(pos,2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "$$PE_{(pos,2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{model}}}\\right)$$\n",
        "\n",
        "\u5176\u4e2d $pos$ \u4e3a\u4f4d\u7f6e\u7d22\u5f15\uff0c$i$ \u4e3a\u7ef4\u5ea6\u7d22\u5f15\u3002\n",
        "\n",
        "### 1.2 \u6a21\u578b\u6982\u8ff0\n",
        "\n",
        "SASRec (Self-Attentive Sequential Recommendation) \u662f\u57fa\u4e8eTransformer\u7684\u81ea\u6ce8\u610f\u529b\u5e8f\u5217\u63a8\u8350\u6a21\u578b\u3002\u5b83\u4f7f\u7528\u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u6765\u6355\u6349\u7528\u6237\u884c\u4e3a\u5e8f\u5217\u4e2d\u7684\u957f\u671f\u4f9d\u8d56\u5173\u7cfb\u3002\n",
        "\n",
        "**\u6838\u5fc3\u7279\u70b9**\uff1a\n",
        "- \u4f7f\u7528\u4f4d\u7f6e\u7f16\u7801\u6355\u6349\u5e8f\u5217\u987a\u5e8f\n",
        "- \u591a\u5934\u6ce8\u610f\u529b\u673a\u5236\u5b66\u4e60\u7269\u54c1\u95f4\u7684\u4f9d\u8d56\n",
        "- \u57fa\u4e8e\u7269\u54c1\u5d4c\u5165\u7684\u9884\u6d4b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a6103d",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_units, num_heads, dropout_rate):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_units // num_heads\n",
        "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to('cuda')\n",
        "\n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        batch_size = query.size(0)\n",
        "        \n",
        "        # Linear projections\n",
        "        Q = self.W_Q(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        K = self.W_K(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        V = self.W_V(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        \n",
        "        # Scaled dot-product attention\n",
        "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
        "        if mask is not None:\n",
        "            attention = attention.masked_fill(mask == 0, -1e9)\n",
        "        attention = self.dropout(F.softmax(attention, dim=-1))\n",
        "        \n",
        "        # Output\n",
        "        output = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\n",
        "        output = output.view(batch_size, -1, self.num_heads * self.head_dim)\n",
        "        output = self.W_O(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e5c20f2",
      "metadata": {},
      "outputs": [],
      "source": [
        "class PointWiseFeedForward(nn.Module):\n",
        "    def __init__(self, hidden_units, dropout_rate):\n",
        "        super(PointWiseFeedForward, self).__init__()\n",
        "        self.W_1 = nn.Linear(hidden_units, hidden_units, bias=True)\n",
        "        self.W_2 = nn.Linear(hidden_units, hidden_units, bias=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        output = self.W_2(F.relu(self.W_1(inputs)))\n",
        "        output = self.dropout(output)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "986838e6",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SASRecBlock(nn.Module):\n",
        "    def __init__(self, hidden_units, num_heads, dropout_rate):\n",
        "        super(SASRecBlock, self).__init__()\n",
        "        self.mha = MultiHeadAttention(hidden_units, num_heads, dropout_rate)\n",
        "        self.ffn = PointWiseFeedForward(hidden_units, dropout_rate)\n",
        "        self.layernorm1 = nn.LayerNorm(hidden_units, eps=1e-8)\n",
        "        self.layernorm2 = nn.LayerNorm(hidden_units, eps=1e-8)\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, input_emb, mask):\n",
        "        attn_output = self.mha(input_emb, input_emb, input_emb, mask)\n",
        "        attn_output = self.dropout1(attn_output)\n",
        "        out1 = self.layernorm1(input_emb + attn_output)\n",
        "        \n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        out2 = self.layernorm2(out1 + ffn_output)\n",
        "        return out2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a236dc2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "class SASRec(nn.Module):\n",
        "    def __init__(self, item_num, hidden_units=50, num_blocks=2, num_heads=1, dropout_rate=0.2, maxlen=100):\n",
        "        super(SASRec, self).__init__()\n",
        "        self.item_num = item_num\n",
        "        self.hidden_units = hidden_units\n",
        "        self.maxlen = maxlen\n",
        "        \n",
        "        self.item_embeddings = nn.Embedding(item_num + 1, hidden_units, padding_idx=0)\n",
        "        self.pos_embeddings = nn.Embedding(maxlen + 1, hidden_units)\n",
        "        \n",
        "        self.blocks = nn.ModuleList([\n",
        "            SASRecBlock(hidden_units, num_heads, dropout_rate) \n",
        "            for _ in range(num_blocks)\n",
        "        ])\n",
        "        \n",
        "        self.LayerNorm = nn.LayerNorm(hidden_units, eps=1e-8)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "        self.apply(self._init_weights)\n",
        "        \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Embedding):\n",
        "            nn.init.normal_(module.weight, mean=0, std=0.01)\n",
        "        elif isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "        if module.bias is not None:\n",
        "            nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, input_seq, mask):\n",
        "        seq_emb = self.item_embeddings(input_seq)\n",
        "        positions = torch.arange(input_seq.size(1), dtype=torch.long, device=input_seq.device)\n",
        "        pos_emb = self.pos_embeddings(positions)\n",
        "        \n",
        "        emb = seq_emb + pos_emb\n",
        "        emb = self.dropout(self.LayerNorm(emb))\n",
        "        \n",
        "        for block in self.blocks:\n",
        "            emb = block(emb, mask)\n",
        "        \n",
        "        return emb\n",
        "    \n",
        "    def predict(self, input_seq, target_items, mask):\n",
        "        seq_emb = self.forward(input_seq, mask)\n",
        "        last_emb = seq_emb[:, -1, :]\n",
        "        target_emb = self.item_embeddings(target_items)\n",
        "        scores = torch.matmul(last_emb, target_emb.transpose(0, 1))\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d7b744c",
      "metadata": {},
      "source": [
        "## 2. TiSASRec\u65f6\u5e8f\u611f\u77e5\u673a\u5236\n",
        "\n",
        "### 2.1 \u65f6\u95f4\u95f4\u9694\u5efa\u6a21\u7406\u8bba\u57fa\u7840\n",
        "\n",
        "TiSASRec\u5728SASRec\u57fa\u7840\u4e0a\u5f15\u5165**\u65f6\u95f4\u95f4\u9694\u4fe1\u606f**\uff0c\u76f8\u90bb\u4ea4\u4e92\u7684\u65f6\u95f4\u95f4\u9694\u88ab\u7f16\u7801\u5e76\u878d\u5165\u6ce8\u610f\u529b\u8ba1\u7b97\u3002\u65f6\u95f4\u95f4\u9694\u7684\u7f16\u7801\u65b9\u5f0f\u5982\u4e0b\uff1a\n",
        "\n",
        "**\u65f6\u95f4\u95f4\u9694\u7f16\u7801\u516c\u5f0f**\uff1a\n",
        "\n",
        "$$t_{ij} = \\log\\left(1 + \\Delta_{ij}\\right) / \\log\\left(1 + T_{max}\\right)$$\n",
        "\n",
        "\u5176\u4e2d\uff1a\n",
        "- $\\Delta_{ij} = |t_i - t_j|$\uff1a\u76f8\u90bb\u4ea4\u4e92\u4e4b\u95f4\u7684\u65f6\u95f4\u5dee\n",
        "- $t_i, t_j$ \u5206\u522b\u4e3a\u7b2c $i$ \u548c $j$ \u4e2a\u4ea4\u4e92\u7684\u65f6\u95f4\u6233\n",
        "- $T_{max}$\uff1a\u9884\u8bbe\u7684\u6700\u5927\u65f6\u95f4\u95f4\u9694\u9608\u503c\uff08\u9ed8\u8ba430\u5929 = 86400\u00d730\u79d2\uff09\n",
        "- $\\log(1 + \\cdot)$\uff1a\u5bf9\u6570\u53d8\u6362\uff0c\u7528\u4e8e\u538b\u7f29\u65f6\u95f4\u8de8\u5ea6\u8303\u56f4\n",
        "\n",
        "**\u5e26\u65f6\u95f4\u95f4\u9694\u7684\u6ce8\u610f\u529b\u8ba1\u7b97**\uff1a\n",
        "\n",
        "$$\\text{Attention}_{time}(Q, K, V, T) = \\text{softmax}\\left(\\frac{QK^T + \\alpha \\cdot T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "\u5176\u4e2d $T$ \u4e3a\u65f6\u95f4\u95f4\u9694\u77e9\u9635\uff0c$\\alpha$ \u4e3a\u65f6\u95f4\u95f4\u9694\u7684\u6743\u91cd\u7cfb\u6570\u3002\n",
        "\n",
        "### 2.2 \u65f6\u95f4\u95f4\u9694\u5efa\u6a21\n",
        "\n",
        "TiSASRec\u5728SASRec\u57fa\u7840\u4e0a\u5f15\u5165\u65f6\u95f4\u95f4\u9694\u4fe1\u606f\uff0c\u76f8\u90bb\u4ea4\u4e92\u7684\u65f6\u95f4\u95f4\u9694\u88ab\u7f16\u7801\u5e76\u878d\u5165\u6ce8\u610f\u529b\u8ba1\u7b97\u3002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7482b0ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_time_matrix(timestamps, max_time_gap=86400 * 30):\n",
        "    \"\"\"\u8ba1\u7b97\u76f8\u90bb\u4ea4\u4e92\u4e4b\u95f4\u7684\u65f6\u95f4\u95f4\u9694\u77e9\u9635\"\"\"\n",
        "    batch_size, seq_len = timestamps.shape\n",
        "    time_diffs = timestamps.unsqueeze(2) - timestamps.unsqueeze(1)\n",
        "    time_diffs = torch.log1p(torch.clamp(time_diffs, min=0)) / (torch.log1p(torch.tensor(max_time_gap)) + 1e-8)\n",
        "    return time_diffs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ad7212a",
      "metadata": {},
      "source": [
        "## 3. mHC\u6d41\u5f62\u7ea6\u675f\u8d85\u8fde\u63a5\n",
        "\n",
        "### 3.1 Sinkhorn-Knopp\u7b97\u6cd5\u7406\u8bba\u57fa\u7840\n",
        "\n",
        "mHC\uff08Manifold Hyperconnection\uff09\u521b\u65b0\u6027\u5730\u5f15\u5165**Sinkhorn-Knopp\u7b97\u6cd5**\uff0c\u5c06\u6743\u91cd\u77e9\u9635\u6295\u5f71\u5230**\u53cc\u968f\u673a\u77e9\u9635\u6d41\u5f62**\u4e0a\uff0c\u5b9e\u73b0\u6d41\u5f62\u7ea6\u675f\u7684\u6b63\u5219\u5316\u3002\n",
        "\n",
        "**\u53cc\u968f\u673a\u77e9\u9635\u5b9a\u4e49**\uff1a\n",
        "\n",
        "\u77e9\u9635 $P \\in \\mathbb{R}^{n \\times n}$ \u88ab\u79f0\u4e3a\u53cc\u968f\u673a\u77e9\u9635\uff0c\u5f53\u4e14\u4ec5\u5f53\uff1a\n",
        "\n",
        "$$P \\mathbf{1} = \\mathbf{1} \\quad \\text{\u4e14} \\quad P^T \\mathbf{1} = \\mathbf{1}$$\n",
        "\n",
        "\u5373\u77e9\u9635\u7684\u6bcf\u4e00\u884c\u548c\u6bcf\u4e00\u5217\u7684\u548c\u5747\u4e3a1\uff0c\u4e14\u6240\u6709\u5143\u7d20\u975e\u8d1f\u3002\n",
        "\n",
        "**Sinkhorn-Knopp\u8fed\u4ee3\u7b97\u6cd5**\uff1a\n",
        "\n",
        "\u901a\u8fc7\u4ea4\u66ff\u7f29\u653e\u884c\u548c\u5217\uff0c\u5c06\u4efb\u610f\u975e\u8d1f\u77e9\u9635 $M$ \u6536\u655b\u5230\u53cc\u968f\u673a\u77e9\u9635 $P$\uff1a\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&\\text{\u521d\u59cb\u5316: } P^{(0)} = M + \\epsilon \\\\\n",
        "&\\text{\u8fed\u4ee3 } k = 1, 2, \\ldots, K: \\\\\n",
        "&\\quad P^{(k)}_{\\text{row}} = \\frac{P^{(k-1)}}{P^{(k-1)} \\mathbf{1} \\mathbf{1}^T} \\quad \\text{\uff08\u884c\u5f52\u4e00\u5316\uff09} \\\\\n",
        "&\\quad P^{(k)} = \\frac{P^{(k)}_{\\text{row}}}{\\mathbf{1} \\mathbf{1}^T P^{(k)}_{\\text{row}}} \\quad \\text{\uff08\u5217\u5f52\u4e00\u5316\uff09}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "\u5176\u4e2d\u5206\u6570\u5f62\u5f0f\u8868\u793a\u9010\u5143\u7d20\u9664\u6cd5\uff0c$\\mathbf{1}$ \u4e3a\u51681\u5411\u91cf\u3002\n",
        "\n",
        "**mHC\u5c42\u7684\u524d\u5411\u4f20\u64ad**\uff1a\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "&H^{(h)} = \\text{Sinkhorn}\\left(W^{(h)}\right) \\quad \\forall h \\in \\{1, \\ldots, h_{num}\\\\\n",
        "&\\text{Head}^{(h)} = X \\cdot H^{(h)} \\\\\n",
        "&Y = \\text{Projection}\\left(\\frac{1}{h_{num}} \\sum_{h=1}^{h_{num}} \\text{Head}^{(h)}\\right) \\\\\n",
        "&\\text{Output} = X + Y\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "### 3.2 Sinkhorn-Knopp\u7b97\u6cd5\n",
        "\n",
        "Sinkhorn-Knopp\u7b97\u6cd5\u901a\u8fc7\u4ea4\u66ff\u7f29\u653e\u884c\u548c\u5217\uff0c\u5c06\u4efb\u610f\u975e\u8d1f\u77e9\u9635\u8f6c\u6362\u4e3a\u53cc\u968f\u673a\u77e9\u9635\u3002"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fcd1a684",
      "metadata": {},
      "outputs": [],
      "source": [
        "def sinkhorn_knopp(M, num_iterations=100, eps=1e-6):\n",
        "    \"\"\"Sinkhorn-Knopp\u7b97\u6cd5\uff1a\u5c06\u77e9\u9635\u6295\u5f71\u5230\u53cc\u968f\u673a\u77e9\u9635\u6d41\u5f62\"\"\"\n",
        "    n = M.shape[0]\n",
        "    P = M + eps\n",
        "    \n",
        "    for _ in range(num_iterations):\n",
        "        row_sums = P.sum(dim=1, keepdim=True)\n",
        "        P = P / (row_sums + 1e-10)\n",
        "        col_sums = P.sum(dim=0, keepdim=True)\n",
        "        P = P / (col_sums + 1e-10)\n",
        "    \n",
        "    return P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ea026ed",
      "metadata": {},
      "outputs": [],
      "source": [
        "class MHCLayer(nn.Module):\n",
        "    def __init__(self, hidden_units, num_heads=4, dropout_rate=0.1):\n",
        "        super(MHCLayer, self).__init__()\n",
        "        self.hidden_units = hidden_units\n",
        "        self.num_heads = num_heads\n",
        "        self.hyper_weights = nn.Parameter(torch.randn(num_heads, hidden_units, hidden_units))\n",
        "        self.out_proj = nn.Linear(hidden_units, hidden_units)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.apply(self._init_weights)\n",
        "    \n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            nn.init.xavier_uniform_(module.weight)\n",
        "            if module.bias is not None:\n",
        "                nn.init.zeros_(module.bias)\n",
        "    \n",
        "    def forward(self, inputs, mask=None):\n",
        "        batch_size, seq_len, hidden_units = inputs.shape\n",
        "        \n",
        "        hyper_proj = []\n",
        "        for h in range(self.num_heads):\n",
        "            W = self.hyper_weights[h]\n",
        "            W_proj = sinkhorn_knopp(W, num_iterations=20)\n",
        "            hyper_proj.append(W_proj)\n",
        "        \n",
        "        head_outputs = []\n",
        "        for h in range(self.num_heads):\n",
        "            output = torch.matmul(inputs, hyper_proj[h])\n",
        "            if mask is not None:\n",
        "                output = output.masked_fill(mask.unsqueeze(-1) == 0, 0)\n",
        "            head_outputs.append(output)\n",
        "        \n",
        "        combined = torch.stack(head_outputs, dim=-1)\n",
        "        combined = combined.mean(dim=-1)\n",
        "        output = self.out_proj(combined)\n",
        "        output = self.dropout(output)\n",
        "        \n",
        "        return inputs + output"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e9af0bc",
      "metadata": {},
      "source": [
        "## 4. \u6838\u5fc3\u4ee3\u7801\u5b9e\u73b0\n",
        "\n",
        "### 4.1 \u6a21\u578b\u7ec4\u4ef6\u5bf9\u6bd4\n",
        "\n",
        "| \u7ec4\u4ef6 | SASRec | TiSASRec | mHC |\n",
        "|------|--------|----------|-----|\n",
        "| \u4f4d\u7f6e\u7f16\u7801 | Learnable | Learnable | Learnable |\n",
        "| \u65f6\u95f4\u7f16\u7801 | None | TimeInterval | None |\n",
        "| \u6ce8\u610f\u529b | MultiHead | MultiHead+Time | MultiHead+mHC |\n",
        "\n",
        "### 4.2 \u8bad\u7ec3\u6d41\u7a0b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bdbd193",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    \n",
        "    for batch in train_loader:\n",
        "        user_ids, seq, pos_items, neg_items, mask = batch\n",
        "        seq = seq.to(device)\n",
        "        pos_items = pos_items.to(device)\n",
        "        neg_items = neg_items.to(device)\n",
        "        mask = mask.to(device)\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        seq_emb = model(seq, mask)\n",
        "        \n",
        "        pos_emb = model.item_embeddings(pos_items)\n",
        "        pos_scores = (seq_emb[:, -1, :] * pos_emb).sum(dim=-1)\n",
        "        \n",
        "        neg_emb = model.item_embeddings(neg_items)\n",
        "        neg_scores = (seq_emb[:, -1, :] * neg_emb).sum(dim=-1)\n",
        "        \n",
        "        loss = criterion(pos_scores, neg_scores)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "    \n",
        "    return total_loss / len(train_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45487c38",
      "metadata": {},
      "source": [
        "## 5. \u6a21\u578b\u5bf9\u6bd4\n",
        "\n",
        "### 5.1 \u6a21\u578b\u7ed3\u6784\u5bf9\u6bd4\n",
        "\n",
        "| \u7ef4\u5ea6 | SASRec | TiSASRec | SASRec+mHC | TiSASRec+mHC |\n",
        "|------|--------|----------|------------|--------------|\n",
        "| \u5d4c\u5165\u7ef4\u5ea6 | 50 | 50 | 50 | 50 |\n",
        "| Transformer\u5c42\u6570 | 2 | 2 | 2 | 2 |\n",
        "| \u6ce8\u610f\u529b\u5934\u6570 | 2 | 2 | 2 | 2 |\n",
        "\n",
        "---\n",
        "\n",
        "**\u4e0a\u4e00\u7ae0**: [01_\u6570\u636e\u4e0e\u5b9e\u9a8c\u8bbe\u8ba1\u62a5\u544a.ipynb](./01_\u6570\u636e\u4e0e\u5b9e\u9a8c\u8bbe\u8ba1\u62a5\u544a.ipynb)  \n",
        "**\u4e0b\u4e00\u7ae0**: [03_\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u62a5\u544a.ipynb](./03_\u8bad\u7ec3\u4e0e\u8bc4\u4f30\u62a5\u544a.ipynb)"
      ]
    }
  ],
  "metadata": {},
  "nbformat": 4,
  "nbformat_minor": 5
}