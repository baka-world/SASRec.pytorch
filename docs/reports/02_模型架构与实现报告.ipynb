{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b24b70b",
   "metadata": {},
   "source": [
    "# 02_模型架构与实现报告\n",
    "\n",
    "**项目名称**: SASRec.pytorch - 基于Transformer的序列推荐系统  \n",
    "**版本**: v1.0  \n",
    "**创建日期**: 2024-01-10  \n",
    "\n",
    "---\n",
    "\n",
    "## 目录\n",
    "\n",
    "1. [SASRec模型架构](#1-SASRec模型架构)  \n",
    "2. [TiSASRec时序感知机制](#2-TiSASRec时序感知机制)  \n",
    "3. [mHC流形约束超连接](#3-mHC流形约束超连接)  \n",
    "4. [核心代码实现](#4-核心代码实现)  \n",
    "5. [模型对比](#5-模型对比)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1. SASRec模型架构\n",
    "\n",
    "### 1.1 模型概述\n",
    "\n",
    "SASRec (Self-Attentive Sequential Recommendation) 是基于Transformer的自注意力序列推荐模型。它使用多头注意力机制来捕捉用户行为序列中的长期依赖关系。\n",
    "\n",
    "**核心特点**：\n",
    "- 使用位置编码捕捉序列顺序\n",
    "- 多头注意力机制学习物品间的依赖\n",
    "- 基于物品嵌入的预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6103d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, hidden_units, num_heads, dropout_rate):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_units // num_heads\n",
    "        self.W_Q = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_K = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_V = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.W_O = nn.Linear(hidden_units, hidden_units, bias=False)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.scale = torch.sqrt(torch.FloatTensor([self.head_dim])).to('cuda')\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_Q(query).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        K = self.W_K(key).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        V = self.W_V(value).view(batch_size, -1, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        attention = torch.matmul(Q, K.permute(0, 1, 3, 2)) / self.scale\n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e9)\n",
    "        attention = self.dropout(F.softmax(attention, dim=-1))\n",
    "        \n",
    "        # Output\n",
    "        output = torch.matmul(attention, V).permute(0, 2, 1, 3).contiguous()\n",
    "        output = output.view(batch_size, -1, self.num_heads * self.head_dim)\n",
    "        output = self.W_O(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5c20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointWiseFeedForward(nn.Module):\n",
    "    def __init__(self, hidden_units, dropout_rate):\n",
    "        super(PointWiseFeedForward, self).__init__()\n",
    "        self.W_1 = nn.Linear(hidden_units, hidden_units, bias=True)\n",
    "        self.W_2 = nn.Linear(hidden_units, hidden_units, bias=True)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        output = self.W_2(F.relu(self.W_1(inputs)))\n",
    "        output = self.dropout(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986838e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRecBlock(nn.Module):\n",
    "    def __init__(self, hidden_units, num_heads, dropout_rate):\n",
    "        super(SASRecBlock, self).__init__()\n",
    "        self.mha = MultiHeadAttention(hidden_units, num_heads, dropout_rate)\n",
    "        self.ffn = PointWiseFeedForward(hidden_units, dropout_rate)\n",
    "        self.layernorm1 = nn.LayerNorm(hidden_units, eps=1e-8)\n",
    "        self.layernorm2 = nn.LayerNorm(hidden_units, eps=1e-8)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, input_emb, mask):\n",
    "        attn_output = self.mha(input_emb, input_emb, input_emb, mask)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(input_emb + attn_output)\n",
    "        \n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236dc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SASRec(nn.Module):\n",
    "    def __init__(self, item_num, hidden_units=50, num_blocks=2, num_heads=1, dropout_rate=0.2, maxlen=100):\n",
    "        super(SASRec, self).__init__()\n",
    "        self.item_num = item_num\n",
    "        self.hidden_units = hidden_units\n",
    "        self.maxlen = maxlen\n",
    "        \n",
    "        self.item_embeddings = nn.Embedding(item_num + 1, hidden_units, padding_idx=0)\n",
    "        self.pos_embeddings = nn.Embedding(maxlen + 1, hidden_units)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            SASRecBlock(hidden_units, num_heads, dropout_rate) \n",
    "            for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "        self.LayerNorm = nn.LayerNorm(hidden_units, eps=1e-8)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Embedding):\n",
    "            nn.init.normal_(module.weight, mean=0, std=0.01)\n",
    "        elif isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "        if module.bias is not None:\n",
    "            nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, input_seq, mask):\n",
    "        seq_emb = self.item_embeddings(input_seq)\n",
    "        positions = torch.arange(input_seq.size(1), dtype=torch.long, device=input_seq.device)\n",
    "        pos_emb = self.pos_embeddings(positions)\n",
    "        \n",
    "        emb = seq_emb + pos_emb\n",
    "        emb = self.dropout(self.LayerNorm(emb))\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            emb = block(emb, mask)\n",
    "        \n",
    "        return emb\n",
    "    \n",
    "    def predict(self, input_seq, target_items, mask):\n",
    "        seq_emb = self.forward(input_seq, mask)\n",
    "        last_emb = seq_emb[:, -1, :]\n",
    "        target_emb = self.item_embeddings(target_items)\n",
    "        scores = torch.matmul(last_emb, target_emb.transpose(0, 1))\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7b744c",
   "metadata": {},
   "source": [
    "## 2. TiSASRec时序感知机制\n",
    "\n",
    "### 2.1 时间间隔建模\n",
    "\n",
    "TiSASRec在SASRec基础上引入时间间隔信息，相邻交互的时间间隔被编码并融入注意力计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7482b0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_time_matrix(timestamps, max_time_gap=86400 * 30):\n",
    "    \"\"\"计算相邻交互之间的时间间隔矩阵\"\"\"\n",
    "    batch_size, seq_len = timestamps.shape\n",
    "    time_diffs = timestamps.unsqueeze(2) - timestamps.unsqueeze(1)\n",
    "    time_diffs = torch.log1p(torch.clamp(time_diffs, min=0)) / (torch.log1p(torch.tensor(max_time_gap)) + 1e-8)\n",
    "    return time_diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad7212a",
   "metadata": {},
   "source": [
    "## 3. mHC流形约束超连接\n",
    "\n",
    "### 3.1 Sinkhorn-Knopp算法\n",
    "\n",
    "Sinkhorn-Knopp算法通过交替缩放行和列，将任意非负矩阵转换为双随机矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd1a684",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sinkhorn_knopp(M, num_iterations=100, eps=1e-6):\n",
    "    \"\"\"Sinkhorn-Knopp算法：将矩阵投影到双随机矩阵流形\"\"\"\n",
    "    n = M.shape[0]\n",
    "    P = M + eps\n",
    "    \n",
    "    for _ in range(num_iterations):\n",
    "        row_sums = P.sum(dim=1, keepdim=True)\n",
    "        P = P / (row_sums + 1e-10)\n",
    "        col_sums = P.sum(dim=0, keepdim=True)\n",
    "        P = P / (col_sums + 1e-10)\n",
    "    \n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea026ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHCLayer(nn.Module):\n",
    "    def __init__(self, hidden_units, num_heads=4, dropout_rate=0.1):\n",
    "        super(MHCLayer, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_heads = num_heads\n",
    "        self.hyper_weights = nn.Parameter(torch.randn(num_heads, hidden_units, hidden_units))\n",
    "        self.out_proj = nn.Linear(hidden_units, hidden_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.apply(self._init_weights)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.zeros_(module.bias)\n",
    "    \n",
    "    def forward(self, inputs, mask=None):\n",
    "        batch_size, seq_len, hidden_units = inputs.shape\n",
    "        \n",
    "        hyper_proj = []\n",
    "        for h in range(self.num_heads):\n",
    "            W = self.hyper_weights[h]\n",
    "            W_proj = sinkhorn_knopp(W, num_iterations=20)\n",
    "            hyper_proj.append(W_proj)\n",
    "        \n",
    "        head_outputs = []\n",
    "        for h in range(self.num_heads):\n",
    "            output = torch.matmul(inputs, hyper_proj[h])\n",
    "            if mask is not None:\n",
    "                output = output.masked_fill(mask.unsqueeze(-1) == 0, 0)\n",
    "            head_outputs.append(output)\n",
    "        \n",
    "        combined = torch.stack(head_outputs, dim=-1)\n",
    "        combined = combined.mean(dim=-1)\n",
    "        output = self.out_proj(combined)\n",
    "        output = self.dropout(output)\n",
    "        \n",
    "        return inputs + output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9af0bc",
   "metadata": {},
   "source": [
    "## 4. 核心代码实现\n",
    "\n",
    "### 4.1 模型组件对比\n",
    "\n",
    "| 组件 | SASRec | TiSASRec | mHC |\n",
    "|------|--------|----------|-----|\n",
    "| 位置编码 | Learnable | Learnable | Learnable |\n",
    "| 时间编码 | None | TimeInterval | None |\n",
    "| 注意力 | MultiHead | MultiHead+Time | MultiHead+mHC |\n",
    "\n",
    "### 4.2 训练流程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdbd193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        user_ids, seq, pos_items, neg_items, mask = batch\n",
    "        seq = seq.to(device)\n",
    "        pos_items = pos_items.to(device)\n",
    "        neg_items = neg_items.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        seq_emb = model(seq, mask)\n",
    "        \n",
    "        pos_emb = model.item_embeddings(pos_items)\n",
    "        pos_scores = (seq_emb[:, -1, :] * pos_emb).sum(dim=-1)\n",
    "        \n",
    "        neg_emb = model.item_embeddings(neg_items)\n",
    "        neg_scores = (seq_emb[:, -1, :] * neg_emb).sum(dim=-1)\n",
    "        \n",
    "        loss = criterion(pos_scores, neg_scores)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45487c38",
   "metadata": {},
   "source": [
    "## 5. 模型对比\n",
    "\n",
    "### 5.1 模型结构对比\n",
    "\n",
    "| 维度 | SASRec | TiSASRec | SASRec+mHC | TiSASRec+mHC |\n",
    "|------|--------|----------|------------|--------------|\n",
    "| 嵌入维度 | 50 | 50 | 50 | 50 |\n",
    "| Transformer层数 | 2 | 2 | 2 | 2 |\n",
    "| 注意力头数 | 2 | 2 | 2 | 2 |\n",
    "\n",
    "---\n",
    "\n",
    "**上一章**: [01_数据与实验设计报告.ipynb](./01_数据与实验设计报告.ipynb)  \n",
    "**下一章**: [03_训练与评估报告.ipynb](./03_训练与评估报告.ipynb)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
