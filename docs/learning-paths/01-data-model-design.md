# SASRec.pytorch 项目学习指南（一）：数据与模型设计

> **目标读者**：完全没有深度学习基础的初学者
> **学习目标**：理解数据格式、模型设计理念、核心算法原理

---

## 第一章：什么是序列推荐？

### 1.1 生活中的推荐系统

你可能遇到过这些场景：

- **淘宝/京东**：首页推荐"你可能喜欢的商品"
- **抖音/B站**：推荐你可能感兴趣的视频
- **网易云音乐**：每日推荐歌曲

这些都是**推荐系统**的应用。

### 1.2 序列推荐的特别之处

传统推荐（协同过滤）：
- 假设用户兴趣是**静态**的
- 只看用户喜欢什么，不考虑时间顺序

序列推荐：
- 认为用户兴趣是**动态变化**的
- 根据用户的历史行为**预测下一步想要什么**

### 1.3 举个例子

用户购物序列：
```
[手机壳, 充电器, 手机膜, 蓝牙耳机]
         ↓
   预测下一个：手机支架？
```

**为什么能预测？**
- 买了手机 → 很可能买手机配件
- 买了手机膜+壳 → 耳机也是配件
- 这就是**序列模式**

---

## 第二章：数据是什么？

### 2.1 原始数据格式

项目使用的数据格式（`data/*.txt`）：

```
用户ID  物品ID  时间戳(可选)
1       5       1609459200
1       7       1609462800
1       3       1609466400
2       10      1609470000
...
```

**解释**：
- **用户ID**：谁在买东西（1号用户）
- **物品ID**：买了什么东西（5号商品）
- **时间戳**：什么时候买的（Unix时间戳）

### 2.2 为什么要处理数据？

原始数据不能直接喂给模型，需要：

1. **排序**：按时间排序用户行为
2. **划分**：
   - 训练集：80% 用户历史
   - 验证集：10% 用于调参
   - 测试集：10% 用于最终评估
3. **编码**：把用户ID、物品ID转成数字

### 2.3 数据处理流程

```
原始数据.txt
    ↓
data_partition()  # 划分训练/验证/测试集
    ↓
用户1: [物品5, 物品7, 物品3, ..., 目标物品]
用户2: [物品10, 物品20, ..., 目标物品]
    ↓
WarpSampler  # 随机采样生成批次
    ↓
(batch_size=128)的小批次数据
    ↓
喂给模型训练
```

### 2.4 你需要理解的关键点

| 概念 | 含义 |
|------|------|
| 用户序列 | 一个用户的所有历史行为，按时间排序 |
| 正样本 | 用户实际交互的下一个物品（正确答案） |
| 负样本 | 用户没有交互的随机物品（干扰项） |
| 批次 | 一次训练处理128个用户的序列 |

---

## 第三章：模型设计理念

### 3.1 整体架构图

```
输入序列: [物品A, 物品B, 物品C, 物品D]
            ↓
         嵌入层  (把物品转成向量)
            ↓
       位置嵌入   (加入位置信息)
            ↓
    ┌────────────────────────┐
    │   Transformer块 × 2    │  ← 核心：自注意力机制
    │   (学习序列模式)       │
    └────────────────────────┘
            ↓
       输出层 (预测下一个物品)
            ↓
     每个物品的打分 [score1, score2, ...]
```

### 3.2 为什么用Transformer？

**传统方法的问题**：
- RNN/LSTM：处理长序列时，早期信息会丢失

**Transformer的优势**：
- 自注意力：任意两个位置都能直接"对话"
- 可以并行计算，速度快

### 3.3 什么是"注意力"？

**生活类比**：
- 读句子"猫坐在垫子上"时
- "猫"这个词会特别关注"坐"、"垫子"
- "在"这个词关注所有词（连接作用）

**在模型中**：
- 每个位置的向量会"关注"其他位置的向量
- 学习哪些位置之间有关系

### 3.4 位置嵌入：为什么需要？

**问题**：模型不知道序列顺序
- 输入 [A, B] 和 [B, A] 对模型来说是一样的
- 但实际上"先买A再买B"和"先买B再买A"完全不同

**解决方案**：位置嵌入
- 位置1：加上位置向量P1
- 位置2：加上位置向量P2
- 这样 [A_P1, B_P2] 和 [B_P1, A_P2] 就不同了

### 3.5 多层堆叠：为什么需要多层？

**类比：读书**
- 第1遍：认识每个字
- 第2遍：理解每个句子
- 第3遍：理解段落大意
- 第4遍：理解全文主题

**模型也是一样**：
- 第1层：学习局部模式（A和B的关系）
- 第2层：学习复杂模式（A和D的关系）
- 第n层：学习抽象模式

---

## 第四章：TiSASRec的特殊设计

### 4.1 什么是TiSASRec？

**TiSASRec = Time Interval Aware Self-Attention**

在SASRec基础上加入**时间间隔**信息。

### 4.2 为什么需要时间信息？

**直觉**：
- 1分钟前买的手机和1个月前买的手机，对预测的影响不同
- 最近的行为应该更重要

**TiSASRec的改进**：
```
SASRec注意力：只考虑物品内容相似度
TiSASRec注意力：物品内容 + 时间间隔
```

### 4.3 时间矩阵

对于序列中的每对物品，计算时间间隔：

```
时间矩阵T[i][j] = |时间i - 时间j|
```

然后把时间间隔转成向量，注入到注意力计算中。

---

## 第五章：mHC是什么？

### 5.1 背景：残差连接

**标准残差连接**：
```
x_{l+1} = x_l + F(x_l)
```
- 把输入直接加到输出上
- 帮助深层网络训练

### 5.2 mHC的改进

**mHC残差连接**：
```
x_{l+1} = H_res × x_l + H_post^T × F(H_pre × x_l)
```

**核心思想**：
- 不是简单相加，而是**加权混合**
- 权重通过学习得到
- 使用数学约束确保稳定

### 5.3 Sinkhorn-Knopp算法

这是mHC的"秘密武器"：

1. 初始化一个矩阵
2. 反复做两件事：
   - 让每行和为1
   - 让每列和为1
3. 最终得到**双随机矩阵**

**作用**：确保信号传播稳定，不会爆炸或消失

### 5.4 mHC的优势

| 方面 | 标准残差 | mHC |
|------|---------|-----|
| 深层网络 | 可能不稳定 | 稳定 |
| 梯度流动 | 简单 | 多路径 |
| 性能 | 好 | 更好 |

---

## 第六章：关键设计决策

### 6.1 超参数设计

| 参数 | 默认值 | 含义 | 为什么选这个 |
|------|--------|------|-------------|
| hidden_units | 50 | 向量维度 | 平衡表达力和速度 |
| num_blocks | 2 | Transformer层数 | 足够学习又不复杂 |
| num_heads | 2 | 注意力头数 | 捕捉多种模式 |
| maxlen | 200 | 序列最大长度 | 平衡长序列和内存 |
| dropout_rate | 0.2 | 随机丢弃比例 | 防止过拟合 |

### 6.2 损失函数设计

**为什么用二元交叉熵？**

训练目标：让正样本得分高，负样本得分低

```
Loss = -log(正样本概率) - log(1 - 负样本概率)
```

**直观理解**：
- 正样本：希望模型打高分 → 损失小
- 负样本：希望模型打低分 → 损失小

---

## 第七章：学习检查点

### 7.1 自我测试

**问题1**：序列推荐和传统推荐有什么不同？

**答案**：序列推荐考虑行为的时间顺序，预测用户下一步想要什么；传统推荐只考虑用户喜欢什么。

---

**问题2**：为什么需要位置嵌入？

**答案**：模型本身不感知序列顺序，需要显式添加位置信息来区分[A,B]和[B,A]。

---

**问题3**：Transformer相比RNN的优势是什么？

**答案**：1) 可以并行计算 2) 任意位置可以直接"对话"，不会丢失长距离依赖。

---

**问题4**：mHC解决了什么问题？

**答案**：在深层网络中，传统残差连接可能不稳定。mHC通过数学约束确保信号稳定传播。

---

### 7.2 推荐阅读顺序

1. 第1章：理解项目要解决什么问题
2. 第2章：理解输入数据格式
3. 第3章：理解模型整体架构
4. 第4-5章：理解两个关键技术（TiSASRec和mHC）
5. 第6章：理解设计决策的原因

### 7.3 进阶学习资源

- **入门视频**：3Blue1Brown的神经网络可视化
- **Transformer论文**：Attention Is All You Need（不用读全部，看简介）
- **SASRec论文**：Self-attentive sequential recommendation

---

## 总结

你现在应该理解：

✅ 什么是序列推荐任务
✅ 数据的格式和处理流程
✅ Transformer的基本原理
✅ TiSASRec如何融入时间信息
✅ mHC如何增强训练稳定性
✅ 模型设计的核心思想

**下一步**：阅读配套的"模型实现"文档，了解代码如何实现这些设计。
